{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Introduction to Pandas: Data Organization and Analysis\n",
    "\n",
    "So far, we have been working with numerical data using NumPy arrays. But what happens when your data is more complex — when it has column headers, mixed types (numbers, categories, text), or comes from a spreadsheet or CSV file with metadata? For these situations, Python has a powerful library called **pandas**.\n",
    "\n",
    "Before we dive in, let's briefly step back and think about **data formats** more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## A Note About Data Formats\n",
    "\n",
    "All data files on your computer are ultimately stored in one of two broad categories:\n",
    "\n",
    "**Human-readable (text) formats:**\n",
    "- Can be opened and inspected with any text editor\n",
    "- Easy to write to and edit by hand\n",
    "- Examples: `.csv`, `.txt`, `.json`, `.xml`\n",
    "- Downside: *large and slow* — storing numbers as text is inefficient\n",
    "\n",
    "**Binary formats:**\n",
    "- Stored in a compact, machine-readable encoding\n",
    "- Faster to read/write, can be compressed, support more features\n",
    "- Examples: `.npy`/`.npz` (NumPy), `.mat` (MATLAB), **HDF5**, **netCDF**\n",
    "- Downside: not human-readable; you need specific software to open them\n",
    "\n",
    "Let's start with the basic tools for reading and writing numerical data files, then work our way up to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8sqnj40jog7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hkz5r6xgyb",
   "metadata": {},
   "source": [
    "## Reading Simple Numerical Data with NumPy\n",
    "\n",
    "If your data is a purely numerical CSV — no header rows, no mixed types, just numbers — `np.loadtxt` is the quickest tool. It reads the file directly into a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hkswdrn4w6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"01HIVseries/HIVseries.csv\"\n",
    "\n",
    "# delimiter=',' tells NumPy that columns are separated by commas\n",
    "data_set = np.loadtxt(data_filename, delimiter=',')\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6tg49gh5o2s",
   "metadata": {},
   "source": [
    "### MATLAB .mat files\n",
    "\n",
    "MATLAB `.mat` files can be read with `scipy.io.loadmat`. The result is a Python **dictionary** whose keys are the variable names from MATLAB and whose values are the corresponding arrays.\n",
    "\n",
    "> **Important note about versions:** After version 7.2, MATLAB switched to an HDF5-based format for `.mat` files. `scipy.io` can only read version 7.2 and earlier. If you are saving a `.mat` file in MATLAB and want to open it in Python with `scipy.io`, use the command `save('data.mat', '-v7')` in MATLAB. For newer `.mat` files you would need the `h5py` package instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yza9hngfwbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadmat returns a dictionary -- keys are the variable names from MATLAB\n",
    "mat_data = io.loadmat(data_filename[:-4] + '.mat')\n",
    "print(mat_data)  # note the metadata keys __header__, __version__, __globals__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11w8wjg6zaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HIV data was stored in a variable called \"a\" in MATLAB\n",
    "hiv_from_mat = mat_data['a']\n",
    "print(hiv_from_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ag7hvlldm",
   "metadata": {},
   "source": [
    "`scipy.io` can also **write** `.mat` files, so you can create data in Python and open it in MATLAB:\n",
    "\n",
    "```python\n",
    "io.savemat('mydata.mat', {'x': x, 'y': y})\n",
    "```\n",
    "\n",
    "### NumPy's Native Binary Format: .npy and .npz\n",
    "\n",
    "NumPy has its own binary file format for saving and loading arrays quickly. It is *not* human-readable — you cannot open it in a text editor — so use it only when you know you'll be working entirely in Python. The upside is speed and exact preservation of dtype (Python data-type).\n",
    "\n",
    "- `np.save('filename', array)` — saves a single array to a `.npy` file\n",
    "- `np.savez('filename', name1=arr1, name2=arr2)` — saves multiple arrays together into a `.npz` (zipped) file; the keyword argument names become the keys you use to retrieve the arrays\n",
    "- `np.load('filename.npy')` — loads a single array back\n",
    "- `np.load('filename.npz')` — loads a `.npz` file into an `NpzFile` object, which works like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xoml7zusis",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1001)\n",
    "y = 3*np.sin(x)**3 - np.sin(x)\n",
    "\n",
    "# Save individually as .npy files\n",
    "np.save('x_values', x)\n",
    "np.save('y_values', y)\n",
    "\n",
    "# Or save both together in a single .npz file\n",
    "# The keyword argument names (x_vals, y_vals) become the dictionary keys\n",
    "np.savez('xy_values', x_vals=x, y_vals=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6wsv2q91nnx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single .npy file -- gives back an array directly\n",
    "x_loaded = np.load('x_values.npy')\n",
    "\n",
    "# Load a .npz file -- gives back an NpzFile object, not an array directly\n",
    "xydata = np.load('xy_values.npz')\n",
    "print('Variables stored in the npz file:', xydata.files)\n",
    "\n",
    "# Access individual arrays using the keyword names as dictionary keys\n",
    "x_from_npz = xydata['x_vals']\n",
    "y_from_npz = xydata['y_vals']\n",
    "print(x_from_npz)\n",
    "print(y_from_npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ep88fzqd3m",
   "metadata": {},
   "source": [
    "Now we have seen the basic toolkit for reading and writing numerical data. But what if your data has column headers, mixed types, or comes from a spreadsheet? That's where **pandas** comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Why Use Pandas?\n",
    "\n",
    "The pandas library is designed to make working with labeled, structured data easy and fast. Here is what it brings to the table:\n",
    "\n",
    "- **Stores data of different types in the same object** — a column of integers next to a column of strings next to a column of dates, all in one structure\n",
    "- **Labels your data** — rows and columns have names, not just integer indices\n",
    "- **Easy save/load from popular file types** — CSV, Excel, HDF5, and more, all with one line of code\n",
    "- **Great organization** — built-in tools for sorting, filtering, and summarizing\n",
    "- **Easy to compute statistics** — mean, std, describe, groupby, and more\n",
    "- **Built-in plotting** — quick visualizations directly from a DataFrame\n",
    "\n",
    "We import pandas with the conventional alias `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## The Two Core Data Structures: Series and DataFrame\n",
    "\n",
    "Pandas is built around two objects:\n",
    "\n",
    "- A **`Series`** is a one-dimensional array with labels (an *index*). Think of it as a single column of data.\n",
    "- A **`DataFrame`** is a two-dimensional table with labeled rows and labeled columns. Think of it as a spreadsheet or a data matrix where **each column is a Series**.\n",
    "\n",
    "Both of these are *mutable* data types.\n",
    "\n",
    "### Creating a Series\n",
    "\n",
    "You can create a `Series` from a list. Pandas assigns integer indices (0, 1, 2, ...) by default, but notice that `np.nan` is allowed as a missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 3, 5, np.nan, 6, 8])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from a Dictionary\n",
    "\n",
    "One of the most common ways to create a `DataFrame` is from a Python dictionary. The keys become the column names, and the values become the columns. **All columns should have the same length** — if you provide a single scalar value, pandas will repeat it to fill the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame with election results data\n",
    "# Notice that we can mix different data types across columns\n",
    "data = {\n",
    "    'State': ['TN', 'TN', 'AL', 'AL', 'GA'],\n",
    "    'District': [1, 2, 1, 2, 1],\n",
    "    'Dem_2020': [35.1, 41.2, 30.5, 38.7, 48.2],\n",
    "    'Rep_2020': [62.4, 56.8, 67.9, 59.8, 49.5],\n",
    "    'Competitive': [False, False, False, False, True]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "Notice how the DataFrame displays nicely as a table with column names and an integer row index (0, 1, 2, ...) on the left. You can also check the data types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each column has its own dtype\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from a NumPy Array\n",
    "\n",
    "You can also create a DataFrame from a NumPy array and supply column names and/or an index (row labels). Here we'll use `pd.date_range` to create a date-based index, which is common when working with time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date index: 6 consecutive days starting 2013-01-01\n",
    "dates = pd.date_range('20130101', periods=6)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 6x4 DataFrame of random data with the date index\n",
    "# and column labels A, B, C, D\n",
    "df_ts = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "**Exercise:** In the cell below, create a DataFrame from a dictionary that represents the following small dataset about three hypothetical SIR model runs. Each run has a `beta` parameter, a `gamma` parameter, a computed basic reproduction number `R0` (recall $R_0 = \\beta/\\gamma$), and a boolean `epidemic` column that is `True` when $R_0 > 1$. Use $\\beta$ values 0.3, 0.5, 0.8 and $\\gamma$ values 0.4, 0.4, 0.3. Compute `R0` and `epidemic` using Python — don't type the values in by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": "beta = np.array([0.3, 0.5, 0.8])\ngamma = np.array([0.4, 0.4, 0.3])\nR0 = beta / gamma\n\nsir_runs = pd.DataFrame({\n    'beta': beta,\n    'gamma': gamma,\n    'R0': R0,\n    'epidemic': R0 > 1\n})\nsir_runs"
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## Viewing Data\n",
    "\n",
    "When you have a large dataset, you won't want to print the whole thing. Pandas gives you several tools to quickly inspect a DataFrame:\n",
    "\n",
    "- `df.head(n)` — shows the first `n` rows (default 5)\n",
    "- `df.tail(n)` — shows the last `n` rows (default 5)\n",
    "- `df.index` — shows the row labels\n",
    "- `df.columns` — shows the column labels\n",
    "- `df.describe()` — shows a quick summary of statistics for numerical columns\n",
    "- `df.T` — transposes the DataFrame (swaps rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the congressional elections data\n",
    "elections = pd.read_csv('Daily_Kos_Elections_08_12_16_congress_districts.csv')\n",
    "\n",
    "# Show just the first 5 rows\n",
    "elections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the last 3 rows\n",
    "elections.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the index and column labels\n",
    "print('Index:', elections.index)\n",
    "print('Columns:', elections.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick statistical summary of all numerical columns\n",
    "elections.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## Getting Data Out: DataFrame.to_numpy()\n",
    "\n",
    "One of the most important things to know is **how to get your data back out into a NumPy array** so you can do mathematics with it. The method `DataFrame.to_numpy()` does this:\n",
    "\n",
    "> **Important:** NumPy arrays have *one* dtype for the entire array, while pandas DataFrames have one dtype *per column*. When you call `to_numpy()`, pandas must find a single dtype that can hold all the columns. If your DataFrame has mixed types (e.g., floats and strings), you'll end up with dtype `object`, which is just a Python object array — not useful for math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ts is all floats, so to_numpy() is fast and clean\n",
    "arr = df_ts.to_numpy()\n",
    "print(type(arr))\n",
    "print(arr.dtype)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a DataFrame with mixed types (like elections), the result is dtype=object\n",
    "# Note: to_numpy() does NOT include the index or column labels\n",
    "arr_elec = elections.to_numpy()\n",
    "print(arr_elec.dtype)\n",
    "print(arr_elec[:3])  # first 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7",
   "metadata": {},
   "source": [
    "When you only need specific numerical columns for computation, it's better to select just those columns before calling `to_numpy()`. We'll see how to do that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## Selecting Subsets of Data\n",
    "\n",
    "This is one of the most important — and most confusing — aspects of pandas. There are several ways to select data, and they behave differently. Let's go through them carefully.\n",
    "\n",
    "We'll use `df_ts` (the random date-indexed DataFrame) for most of these examples. If it's been a while, run the cell below to recreate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate df_ts with a fixed random seed so everyone gets the same data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('20130101', periods=6)\n",
    "df_ts = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "### Rule 1: Single `[]` with a string selects a **column** (returns a Series)\n",
    "\n",
    "When you use `df['column_name']`, you get that column as a `Series`. You can also use `df.column_name` as a shorthand (but `[]` notation is safer because it works even when the column name conflicts with a pandas method name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column -- gives back a Series\n",
    "col_A = df_ts['A']\n",
    "print(type(col_A))\n",
    "print(col_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "### Rule 2: Single `[]` with a **slice** selects **rows**\n",
    "\n",
    "When you use `df[0:3]` or `df['20130102':'20130104']`, you get rows. Note: this is the *opposite* of Rule 1! Slicing with `[]` gives rows, not columns. This inconsistency is a genuine gotcha — which is why the `.loc` and `.iloc` methods (below) are preferred.\n",
    "\n",
    "The idea is that you typically want to select single columns but multiple rows, but of course this isn't always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing by integer position gives rows\n",
    "print(df_ts[0:3])  # first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing by label gives rows (and both endpoints are INCLUDED)\n",
    "# THIS WILL BE A GENERAL RULE: slicing by label includes the endpoint, while slicing by integer position does NOT include the endpoint.\n",
    "print(df_ts['20130102':'20130104'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-2",
   "metadata": {},
   "source": [
    "### Rule 3: `.loc[]` — selection by **label**\n",
    "\n",
    "`.loc[row_label, column_label]` selects data by the *name* of the row and column. This is the preferred method when your index has meaningful labels (like dates or strings).\n",
    "\n",
    "> **Note:** With `.loc`, label slicing **includes both endpoints** (unlike Python slices which exclude the right endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single row by its label (returns a Series)\n",
    "df_ts.loc[dates[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows (:) but only columns A and B\n",
    "df_ts.loc[:, ['A', 'B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice rows by label AND select specific columns\n",
    "# Note: both date endpoints are included!\n",
    "df_ts.loc['20130102':'20130104', ['A', 'B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single scalar value by row and column label\n",
    "df_ts.loc[dates[0], 'A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-2",
   "metadata": {},
   "source": [
    "### Rule 4: `.iloc[]` — selection by **integer position**\n",
    "\n",
    "`.iloc[row_position, column_position]` selects data by *integer index*, just like NumPy array indexing. Positions are 0-based, and slices **exclude** the right endpoint (standard Python behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row at integer position 3 (the 4th row)\n",
    "df_ts.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 3 and 4 (positions 3:5), columns 0 and 1 (positions 0:2)\n",
    "# Right endpoint is EXCLUDED, just like NumPy\n",
    "df_ts.iloc[3:5, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific row and column positions using lists\n",
    "df_ts.iloc[[1, 2, 4], [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single scalar by position\n",
    "df_ts.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-2",
   "metadata": {},
   "source": [
    "### Summary: When to use what\n",
    "\n",
    "| Syntax | What it does | Notes |\n",
    "|---|---|---|\n",
    "| `df['col']` | Select a column (→ Series) | Use for single column |\n",
    "| `df[0:3]` | Slice rows by position | Use for a block of rows |\n",
    "| `df.loc[row, col]` | Select by **label** | Both endpoints included in slices |\n",
    "| `df.iloc[row, col]` | Select by **integer position** | Excludes right endpoint (like NumPy) |\n",
    "\n",
    "**Exercise:** Using the `elections` DataFrame and `.loc` or `.iloc`, answer the following:\n",
    "\n",
    "1. In the cell below, select only the `'Clinton\\r\\n2016'` and `'Trump\\r\\n2016'` columns for all rows. Save the result to a variable and print the first 5 rows.\n",
    "2. Then convert it to a NumPy array and compute the mean vote share for each candidate across all districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6-2",
   "metadata": {},
   "outputs": [],
   "source": "# Part 1: select two columns\n# Note: the column names contain a carriage return + newline (\\r\\n) from the CSV header\nclinton_trump = elections[['Clinton\\r\\n2016', 'Trump\\r\\n2016']]\nprint(clinton_trump.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7-2",
   "metadata": {},
   "outputs": [],
   "source": "# Part 2: convert to numpy and compute means\nct_arr = clinton_trump.to_numpy()\nprint('Mean Clinton 2016 vote share: {:.2f}%'.format(ct_arr[:, 0].mean()))\nprint('Mean Trump 2016 vote share:   {:.2f}%'.format(ct_arr[:, 1].mean()))"
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-2",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "\n",
    "Just like NumPy, you can use a boolean condition to filter rows. This is extremely useful for finding data that satisfies some criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows where column A is positive\n",
    "df_ts[df_ts['A'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a condition across the whole DataFrame:\n",
    "# values that do NOT satisfy the condition become NaN\n",
    "df_ts[df_ts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with the elections data:\n",
    "# Find all districts where Clinton got more than 50% of the vote in 2016\n",
    "clinton_majority = elections[elections['Clinton\\r\\n2016'] > 50]\n",
    "print(f\"Number of districts where Clinton got >50%: {len(clinton_majority)}\")\n",
    "clinton_majority.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2-2",
   "metadata": {},
   "source": [
    "**Exercise:** In the cell below, use boolean indexing on the `elections` DataFrame to find all districts where the Republican candidate got more than 70% of the vote in 2016. How many such districts are there? Print the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3-2",
   "metadata": {},
   "outputs": [],
   "source": "gop_landslide = elections[elections['Trump\\r\\n2016'] > 70]\nprint(f\"Districts where Trump got >70%: {len(gop_landslide)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-2",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "You can sort a DataFrame by the values in one or more columns using `sort_values()`, or by the index using `sort_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the elections data by Clinton's 2016 vote share, highest first\n",
    "elections.sort_values(by='Clinton\\r\\n2016', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df_ts by column B\n",
    "df_ts.sort_values(by='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by index (rows) in reverse order\n",
    "df_ts.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8-2",
   "metadata": {},
   "source": [
    "## Operations and Statistics\n",
    "\n",
    "Pandas makes it easy to compute descriptive statistics. By default, operations **skip missing values** (`NaN`). Operations apply column-by-column (i.e., down each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of each column\n",
    "print('Column means:')\n",
    "print(df_ts.mean())\n",
    "\n",
    "print('\\nRow means (axis=1 means operate along columns, i.e., across each row):')\n",
    "print(df_ts.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary statistics\n",
    "df_ts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can apply numpy functions too\n",
    "print('Standard deviation of each column:')\n",
    "print(df_ts.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2-2",
   "metadata": {},
   "source": [
    "**Exercise:** Use the `elections` DataFrame to answer the following:\n",
    "\n",
    "1. What was the mean vote share for Obama in 2012 across all congressional districts?\n",
    "2. What was the standard deviation?\n",
    "3. In how many districts did Obama get more than his mean vote share? (Use boolean indexing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3-2",
   "metadata": {},
   "outputs": [],
   "source": "obama_mean = elections['Obama\\r\\n2012'].mean()\nobama_std  = elections['Obama\\r\\n2012'].std()\nprint(f\"Mean Obama 2012 vote share:  {obama_mean:.2f}%\")\nprint(f\"Std dev:                     {obama_std:.2f}%\")\n\nabove_mean = elections[elections['Obama\\r\\n2012'] > obama_mean]\nprint(f\"Districts above mean:        {len(above_mean)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-3",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Pandas represents missing data as `np.nan`. Many operations ignore missing data by default. There are several tools for dealing with it:\n",
    "\n",
    "- `df.dropna(how='any')` — drop any row that has at least one `NaN`\n",
    "- `df.dropna(how='all')` — drop rows where *all* values are `NaN`\n",
    "- `df.fillna(value)` — fill `NaN` values with a specified value\n",
    "- `pd.isna(df)` — return a boolean DataFrame showing where the `NaN`s are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with some missing values for demonstration\n",
    "df_missing = df_ts.copy()\n",
    "df_missing.iloc[0, 2] = np.nan   # row 0, column C\n",
    "df_missing.iloc[3, 0] = np.nan   # row 3, column A\n",
    "df_missing.iloc[5, :] = np.nan   # all of row 5\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where the NaNs are\n",
    "pd.isna(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row with at least one NaN\n",
    "df_missing.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with 0\n",
    "df_missing.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-3",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Pandas DataFrames and Series have a built-in `.plot()` method that calls matplotlib under the hood. It automatically uses column names as labels, which is very convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a cumulative sum random walk (one column = one line)\n",
    "df_walk = pd.DataFrame(\n",
    "    np.random.randn(200, 4),\n",
    "    columns=['A', 'B', 'C', 'D']\n",
    ").cumsum()\n",
    "\n",
    "plt.figure()\n",
    "df_walk.plot()\n",
    "plt.title('Random Walks')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Position')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Clinton vote share across all districts\n",
    "plt.figure()\n",
    "elections['Clinton\\r\\n2016'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "plt.xlabel('Vote share (%)')\n",
    "plt.title('Clinton 2016 vote share by congressional district')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-3",
   "metadata": {},
   "source": [
    "**Exercise:** Make a scatter plot of Clinton's 2016 vote share (x-axis) vs. Obama's 2012 vote share (y-axis). Use `kind='scatter'` and set appropriate axis labels. What pattern do you see? Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-3",
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nelections.plot(kind='scatter', x='Clinton\\r\\n2016', y='Obama\\r\\n2012', alpha=0.4)\nplt.xlabel(\"Clinton 2016 vote share (%)\")\nplt.ylabel(\"Obama 2012 vote share (%)\")\nplt.title(\"Clinton 2016 vs. Obama 2012 by congressional district\")\nplt.tight_layout()\nplt.show()\n# The two are strongly positively correlated -- districts that leaned Democratic\n# in 2012 generally stayed Democratic in 2016, though Clinton underperformed\n# Obama in many districts (the cloud sits slightly below the diagonal)."
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-3",
   "metadata": {},
   "source": [
    "## Importing and Exporting Data\n",
    "\n",
    "One of pandas' greatest strengths is the ease with which you can read and write data in a variety of formats. The general pattern is:\n",
    "\n",
    "- **Read:** `pd.read_csv()`, `pd.read_excel()`, `pd.read_hdf()`, `pd.read_json()`, ...\n",
    "- **Write:** `df.to_csv()`, `df.to_excel()`, `df.to_hdf()`, `df.to_json()`, ...\n",
    "\n",
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df_ts to a CSV file\n",
    "df_ts.to_csv('demo_output.csv')\n",
    "\n",
    "# Read it back\n",
    "df_reload = pd.read_csv('demo_output.csv', index_col=0)\n",
    "df_reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-3",
   "metadata": {},
   "source": [
    "The `index_col=0` argument tells pandas that the first column of the CSV is the row index, not a data column. Without it, you'd get an extra unnamed column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7-3",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "HDF5 is an efficient binary format well-suited for large datasets. It requires the `tables` package (`conda install pytables`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDF5 (requires pytables / tables package)\n",
    "# df_ts.to_hdf('demo.h5', key='timeseries')\n",
    "# pd.read_hdf('demo.h5', 'timeseries')\n",
    "\n",
    "# We'll skip the actual execution here since pytables may not be installed,\n",
    "# but the syntax is exactly the same pattern as CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-3",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Excel (requires openpyxl: conda install openpyxl)\n",
    "# df_ts.to_excel('demo.xlsx', sheet_name='Sheet1')\n",
    "# pd.read_excel('demo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])\n",
    "\n",
    "# Again, same pattern — just a different function name and file extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-3",
   "metadata": {},
   "source": [
    "### Reading the HIV Data Again — Now with Pandas\n",
    "\n",
    "Earlier we loaded the HIV CSV with `np.loadtxt`. Pandas can read the same file and add column labels automatically, making the data much easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HIV data with pandas and add meaningful column names\n",
    "hiv = pd.read_csv('01HIVseries/HIVseries.csv', header=None,\n",
    "                  names=['time_years', 'viral_load'])\n",
    "hiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy to plot directly from the DataFrame\n",
    "plt.figure()\n",
    "plt.plot(hiv['time_years'], hiv['viral_load'], 'o-')\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Viral load')\n",
    "plt.title('HIV Viral Load Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the data out as a numpy array for math:\n",
    "t = hiv['time_years'].to_numpy()\n",
    "V = hiv['viral_load'].to_numpy()\n",
    "print('Time array:', t)\n",
    "print('Max viral load:', V.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5-3",
   "metadata": {},
   "source": [
    "## Gotchas: The Truth Value of a Series\n",
    "\n",
    "Here is a common error that you will almost certainly encounter. Suppose you try to use a boolean condition on an entire array or Series inside an `if` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This causes the same kind of error in plain NumPy:\n",
    "A = np.random.rand(8)\n",
    "# Uncomment the line below to see the error:\n",
    "# if A > 0.5:\n",
    "#     print('bigger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7-3",
   "metadata": {},
   "source": [
    "The error message you get is:\n",
    "\n",
    "```\n",
    "ValueError: The truth value of an array is ambiguous. Use a.any() or a.all()\n",
    "```\n",
    "\n",
    "The problem is that `A > 0.5` produces a boolean **array**, not a single `True` or `False`. Python's `if` statement needs a single boolean. The fix is to use `.any()` (at least one element satisfies the condition) or `.all()` (all elements satisfy the condition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(8)\n",
    "print('A =', A)\n",
    "\n",
    "# Correct: are ANY elements bigger than 0.5?\n",
    "if (A > 0.5).any():\n",
    "    print('At least one element is bigger than 0.5')\n",
    "\n",
    "# Correct: are ALL elements bigger than 0.5?\n",
    "if (A > 0.5).all():\n",
    "    print('All elements are bigger than 0.5')\n",
    "else:\n",
    "    print('Not all elements are bigger than 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9-3",
   "metadata": {},
   "source": [
    "The same fix works for a pandas Series or DataFrame. This is one of the most common errors beginners encounter in both NumPy and pandas — now you know how to handle it!\n",
    "\n",
    "Similarly, when you want to combine two boolean conditions in pandas, use `&` (and) or `|` (or) instead of `and`/`or`, and **wrap each condition in parentheses**. This operates elementwise instead of returning an error due to the fact that `and`/`or` are expecting single values on each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Districts where Clinton got >45% AND Trump got <50%\n",
    "# Each condition MUST be in parentheses when combining with & or |\n",
    "close_races = elections[(elections['Clinton\\r\\n2016'] > 45) & (elections['Trump\\r\\n2016'] < 50)]\n",
    "print(f\"Close races (Clinton > 45% and Trump < 50%): {len(close_races)} districts\")\n",
    "close_races.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1-3",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Here is a small worked example that ties together many of the concepts from this notebook. We will use the congressional elections data to look at how Democratic vote share changed from 2008 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so we don't modify the original\n",
    "df_analysis = elections.copy()\n",
    "\n",
    "# Add a new column: change in Democratic vote share from 2008 to 2016\n",
    "df_analysis['Dem_change'] = df_analysis['Clinton\\r\\n2016'] - df_analysis['Obama\\r\\n2008']\n",
    "\n",
    "# Summary statistics for this new column\n",
    "print('Change in Democratic vote share (2008 → 2016):')\n",
    "print(df_analysis['Dem_change'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which districts saw the largest swings toward Democrats?\n",
    "df_analysis.sort_values('Dem_change', ascending=False)[['CD', 'Incumbent', 'Dem_change']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which districts swung most toward Republicans?\n",
    "df_analysis.sort_values('Dem_change', ascending=True)[['CD', 'Incumbent', 'Dem_change']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the swing\n",
    "plt.figure()\n",
    "df_analysis['Dem_change'].plot(kind='hist', bins=40, edgecolor='black')\n",
    "plt.axvline(0, color='red', linestyle='--', label='No change')\n",
    "plt.xlabel('Change in Democratic vote share (2008 → 2016, %)')\n",
    "plt.title('How did districts shift between 2008 and 2016?')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-4",
   "metadata": {},
   "source": [
    "**Final exercise:** Repeat the analysis above for the Republican side — compute the change in Republican vote share from 2008 to 2016, then find the 5 districts with the largest Republican swings and the 5 districts with the largest Democratic swings. Plot a histogram. Does the distribution look like what you'd expect given the Democratic one above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-4",
   "metadata": {},
   "outputs": [],
   "source": "df_analysis['Rep_change'] = df_analysis['Trump\\r\\n2016'] - df_analysis['Romney\\r\\n2012']\n\nprint('Change in Republican vote share (2012 → 2016):')\nprint(df_analysis['Rep_change'].describe())\n\nprint('\\n5 districts with largest Republican swing (toward Trump):')\nprint(df_analysis.sort_values('Rep_change', ascending=False)[['CD', 'Incumbent', 'Rep_change']].head(5).to_string())\n\nprint('\\n5 districts with largest Democratic swing (away from Trump):')\nprint(df_analysis.sort_values('Rep_change', ascending=True)[['CD', 'Incumbent', 'Rep_change']].head(5).to_string())\n\nplt.figure()\ndf_analysis['Rep_change'].plot(kind='hist', bins=40, edgecolor='black')\nplt.axvline(0, color='red', linestyle='--', label='No change')\nplt.xlabel('Change in Republican vote share (2012 → 2016, %)')\nplt.title('How did districts shift between 2012 and 2016?')\nplt.legend()\nplt.show()\n# The Republican distribution is roughly the mirror image of the Democratic one --\n# Trump gained in most districts where Clinton lost, and vice versa,\n# as expected in a two-party system where votes are roughly conserved."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}